{
  "name" : "Kasper Janssens",
  "personal" : {
     "address" : "Ottergemsesteenweg 354, 9000 Gent, Belgium",
     "date of birth" : "12 June 1981"
  },
  "telephone" : "+32 495 61 45 92",
  "email" : "kasper@krondorsoft.com",
  "website" : "www.krondorsoft.com",
  "education" : {
    "degree" : "Civil engineer in computer sciences, major in computer architecture",
    "from" : "01/10/1999",
    "to": "30/06/2004",
    "school" : "University of Ghent",
    "these" : ["I implemented a firewall on the Intel IXP1200 network process, using Netfilter as a guidance beacon. This processor has six cores that do the work, and one core that controls the other six.", "Each of the cores can run four threads, with easy context switching, one clock cycle, through a small amount of registers. This unfortunately limits the amount of variables used in programs, making very careful programming an absolute necessity.", "The controlling core is responsible for handling all exceptions and tasks and is programmed in C. The other processors were programmed in assembler. "]
  },
  "bio" : "I am an experienced developer with a fondness for strongly typed, compiled languages. I focus on application design and testability.",
  "hobbies" : ["Tango", "Salsa", "Programming", "Reading", "Gaming", "Gardening"],
  "experiences": {
      "Western Digital" : {
	"description" : "Amplidata is a startup in the big data storage sector. They have developed a software product to store and retrieve data at high speeds and high safety, meaning that the chance of losing data as a cause of disasters is nearly non-existent. Amplidata got acquired by Western Digital March 2015. The architecture is that of a large distributed system, with lots of smaller servers needing to work together, perform master election, achieve high throughput through utilising multiple machines and their full network bandwidth. ",
	"roles" : [
        {
           "title" : "Streaming Containerization in Java",
           "from" : "01/07/2018",
           "to" : "04/06/2019",
           "description" : ["Like a lot of the object storage architectures, disaster recovery tends to be proportional to the size of the metadata and as such the size of the managed objects. The goal of this project is to keep the amount of managed objects small through containerization, but of course still being able to retrieve the smaller objects. Targets got achieved with minimal latency.","Java, rx-java, parallel and concurrent programming, maven, clover, jenkins, junit, gRPC"]
        },
        {
           "title" : "Java Hadoop HDFS developer",
           "from" : "01/09/2016",
           "to" : "30/06/2018",
           "description" : ["Because of customer requirements to integrate the data storage with Hadoop we started development, together with a team at Microsoft, to write a HDFS extension so we could attach one of our storage systems to a datanode and use the data present in the storage system in Hadoop calculations. Contributed to the open source project, contributions merged in to trunk of Hadoop", "Java, maven, junit, gRPC, Avro, Hadoop"]
        },
        {
           "title" : "Haskell developer",
           "from" : "01/09/2014",
           "to" : "01/09/2016",
           "description" : [" Development of the next-gen storage product (the one that used to be written in OCaml). Due to the high parallelism and fast io subsystem in the language we chose to write it in Haskell.", "Haskell, mio, distributed programming, high troughput, resource management"]
        },
        {
           "title": "OCaml developer",
           "from": "21/05/2013",
           "to": "01/09/2014",
           "description": ["The core product, the object store. The main difficulties of the software lie in the technical complexity of the software itself, multiprocessing, high parallellism and the very high impact of bugs. Achieved performance goals and hardened the product.", "OCaml, ounit, coq, distributed programming, high throughput"]
        }
      ]
     }
  }
}

